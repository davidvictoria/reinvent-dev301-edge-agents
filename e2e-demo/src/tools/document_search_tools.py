"""Document search tools for local RAG using ChromaDB and Ollama embeddings.

Provides tools for indexing and searching technical documentation locally
using vector embeddings generated by Ollama's nomic-embed-text model.
"""

import os
import hashlib
import re
from pathlib import Path
from typing import List, Optional, Dict, Any

import chromadb
from chromadb.api.types import EmbeddingFunction, Documents, Embeddings
import requests

from strands import tool

from ..models.document_chunk import DocumentChunk


class OllamaEmbeddingFunction(EmbeddingFunction):
    """ChromaDB embedding function using Ollama's embedding models.
    
    Generates embeddings using a local Ollama instance with the specified
    embedding model (default: nomic-embed-text).
    
    Attributes:
        model: The Ollama model name for embeddings
        host: The Ollama server URL
    """
    
    def __init__(
        self, 
        model: str = "nomic-embed-text",
        host: str = "http://localhost:11434"
    ):
        """Initialize the Ollama embedding function.
        
        Args:
            model: Ollama embedding model name
            host: Ollama server URL
        """
        self.model = model
        self.host = host.rstrip("/")
    
    def __call__(self, input: Documents) -> Embeddings:
        """Generate embeddings for a list of documents.
        
        Args:
            input: List of text documents to embed
            
        Returns:
            List of embedding vectors
        """
        embeddings = []
        for text in input:
            embedding = self._get_embedding(text)
            embeddings.append(embedding)
        return embeddings
    
    def _get_embedding(self, text: str) -> List[float]:
        """Get embedding for a single text using Ollama API.
        
        Args:
            text: Text to embed
            
        Returns:
            Embedding vector as list of floats
        """
        url = f"{self.host}/api/embeddings"
        payload = {
            "model": self.model,
            "prompt": text
        }
        
        try:
            response = requests.post(url, json=payload, timeout=30)
            response.raise_for_status()
            result = response.json()
            return result.get("embedding", [])
        except requests.RequestException as e:
            # Return empty embedding on error - ChromaDB will handle this
            print(f"Warning: Failed to get embedding from Ollama: {e}")
            return []


class DocumentSearchTools:
    """Tools for local document search using vector embeddings.
    
    Provides document indexing and semantic search capabilities using
    ChromaDB for vector storage and Ollama for embedding generation.
    
    Attributes:
        storage_path: Path to ChromaDB persistent storage
        embedding_model: Ollama model name for embeddings
        ollama_host: Ollama server URL
        client: ChromaDB persistent client
        collection: ChromaDB collection for documents
    """
    
    def __init__(
        self,
        storage_path: str = "./vector_store",
        embedding_model: str = "nomic-embed-text",
        ollama_host: str = "http://localhost:11434"
    ):
        """Initialize document search tools.
        
        Args:
            storage_path: Directory path for ChromaDB storage
            embedding_model: Ollama model for embeddings
            ollama_host: Ollama server URL
        """
        self.storage_path = storage_path
        self.embedding_model = embedding_model
        self.ollama_host = ollama_host
        
        # Ensure storage directory exists
        os.makedirs(storage_path, exist_ok=True)
        
        # Initialize ChromaDB with persistent storage
        self.client = chromadb.PersistentClient(path=storage_path)
        
        # Create embedding function
        self.embedding_function = OllamaEmbeddingFunction(
            model=embedding_model,
            host=ollama_host
        )
        
        # Get or create the documents collection
        self.collection = self.client.get_or_create_collection(
            name="documents",
            embedding_function=self.embedding_function,
            metadata={"hnsw:space": "cosine"}
        )
    
    def _chunk_text(self, text: str, chunk_size: int = 500) -> List[str]:
        """Split text into chunks by paragraphs or sections.
        
        Args:
            text: The text to chunk
            chunk_size: Target size for each chunk in characters
            
        Returns:
            List of text chunks
        """
        # First, try to split by double newlines (paragraphs)
        paragraphs = re.split(r'\n\s*\n', text)
        
        chunks = []
        current_chunk = ""
        
        for para in paragraphs:
            para = para.strip()
            if not para:
                continue
                
            # If adding this paragraph would exceed chunk size, save current and start new
            if current_chunk and len(current_chunk) + len(para) + 2 > chunk_size:
                chunks.append(current_chunk.strip())
                current_chunk = para
            else:
                if current_chunk:
                    current_chunk += "\n\n" + para
                else:
                    current_chunk = para
        
        # Don't forget the last chunk
        if current_chunk.strip():
            chunks.append(current_chunk.strip())
        
        # If no chunks were created (single block of text), split by sentences
        if not chunks and text.strip():
            chunks = [text.strip()]
        
        return chunks
    
    def _read_file(self, file_path: str) -> Optional[str]:
        """Read content from a file based on its format.
        
        Supports TXT, Markdown, and PDF formats.
        
        Args:
            file_path: Path to the file to read
            
        Returns:
            File content as string, or None if unsupported/error
        """
        path = Path(file_path)
        
        if not path.exists():
            return None
        
        suffix = path.suffix.lower()
        
        if suffix in ['.txt', '.md', '.markdown']:
            try:
                with open(path, 'r', encoding='utf-8') as f:
                    return f.read()
            except Exception as e:
                print(f"Error reading file {file_path}: {e}")
                return None
        
        elif suffix == '.pdf':
            # Try to use PyPDF2 if available
            try:
                import PyPDF2
                with open(path, 'rb') as f:
                    reader = PyPDF2.PdfReader(f)
                    text_parts = []
                    for page in reader.pages:
                        text_parts.append(page.extract_text())
                    return "\n\n".join(text_parts)
            except ImportError:
                print("Warning: PyPDF2 not installed. Cannot read PDF files.")
                return None
            except Exception as e:
                print(f"Error reading PDF {file_path}: {e}")
                return None
        
        return None
    
    def _generate_chunk_id(self, document_path: str, chunk_index: int) -> str:
        """Generate a unique ID for a document chunk.
        
        Args:
            document_path: Path to the source document
            chunk_index: Index of the chunk within the document
            
        Returns:
            Unique chunk ID string
        """
        content = f"{document_path}:{chunk_index}"
        return hashlib.md5(content.encode()).hexdigest()
    
    def index_document(self, file_path: str) -> str:
        """Index a document for semantic search.
        
        Reads the document, chunks it, generates embeddings, and stores
        in the vector database.
        
        Args:
            file_path: Path to the document to index
            
        Returns:
            Status message indicating success or failure
        """
        path = Path(file_path)
        
        # Check file exists
        if not path.exists():
            return f"Error: File not found: {file_path}"
        
        # Check supported format
        supported_formats = ['.txt', '.md', '.markdown', '.pdf']
        if path.suffix.lower() not in supported_formats:
            return (
                f"Error: Unsupported file format '{path.suffix}'. "
                f"Supported formats: {', '.join(supported_formats)}"
            )
        
        # Read file content
        content = self._read_file(file_path)
        if content is None:
            return f"Error: Failed to read file: {file_path}"
        
        if not content.strip():
            return f"Error: File is empty: {file_path}"
        
        # Chunk the content
        chunks = self._chunk_text(content)
        
        if not chunks:
            return f"Error: No content chunks generated from: {file_path}"
        
        # Prepare data for ChromaDB
        ids = []
        documents = []
        metadatas = []
        
        for i, chunk_content in enumerate(chunks):
            chunk_id = self._generate_chunk_id(str(path.absolute()), i)
            ids.append(chunk_id)
            documents.append(chunk_content)
            metadatas.append({
                "document_path": str(path.absolute()),
                "document_name": path.name,
                "chunk_index": i,
                "total_chunks": len(chunks)
            })
        
        # Add to collection (upsert to handle re-indexing)
        try:
            self.collection.upsert(
                ids=ids,
                documents=documents,
                metadatas=metadatas
            )
        except Exception as e:
            return f"Error: Failed to index document: {e}"
        
        return (
            f"Successfully indexed document: {path.name}\n"
            f"  Path: {path.absolute()}\n"
            f"  Chunks created: {len(chunks)}\n"
            f"  Total characters: {len(content)}"
        )
    
    def search_documents(self, query: str, n_results: int = 5) -> str:
        """Search technical documentation for relevant information.
        
        Performs semantic similarity search against indexed documents.
        
        Args:
            query: The search query
            n_results: Maximum number of results to return
            
        Returns:
            Formatted string with search results or suggestions
        """
        if not query.strip():
            return "Error: Search query cannot be empty."
        
        # Check if collection has any documents
        count = self.collection.count()
        if count == 0:
            return (
                "No documents have been indexed yet.\n"
                "Use the index_document tool to add documents to the search index."
            )
        
        try:
            results = self.collection.query(
                query_texts=[query],
                n_results=min(n_results, count)
            )
        except Exception as e:
            return f"Error: Search failed: {e}"
        
        # Check if we got any results
        if not results['documents'] or not results['documents'][0]:
            return (
                f"No relevant documents found for query: '{query}'\n\n"
                "Suggestions:\n"
                "  - Try using different keywords\n"
                "  - Use more specific technical terms\n"
                "  - Check if relevant documents have been indexed"
            )
        
        # Format results
        output_lines = [
            f"Search Results for: '{query}'",
            "=" * 50
        ]
        
        documents = results['documents'][0]
        metadatas = results['metadatas'][0]
        distances = results.get('distances', [[]])[0]
        
        for i, (doc, meta) in enumerate(zip(documents, metadatas), 1):
            doc_name = meta.get('document_name', 'Unknown')
            chunk_idx = meta.get('chunk_index', 0)
            total_chunks = meta.get('total_chunks', 1)
            
            # Calculate relevance score (convert distance to similarity)
            relevance = ""
            if distances and i <= len(distances):
                # ChromaDB returns distances, lower is better for cosine
                score = 1 - distances[i-1] if distances[i-1] <= 1 else 0
                relevance = f" (relevance: {score:.2%})"
            
            output_lines.append(f"\n--- Result {i}{relevance} ---")
            output_lines.append(f"Source: {doc_name} (chunk {chunk_idx + 1}/{total_chunks})")
            output_lines.append(f"Content:\n{doc[:500]}{'...' if len(doc) > 500 else ''}")
        
        output_lines.append("\n" + "=" * 50)
        output_lines.append(f"Found {len(documents)} relevant chunk(s)")
        
        return "\n".join(output_lines)
    
    def get_tools(self) -> List:
        """Get the document search tools as decorated functions.
        
        Returns:
            List of tool functions for use with the agent
        """
        @tool
        def search_documents(query: str, n_results: int = 5) -> str:
            """Search technical documentation for relevant information.
            
            Performs semantic similarity search against indexed documents
            to find relevant technical information, troubleshooting guides,
            and equipment specifications.
            
            Args:
                query: The search query describing what information you need
                n_results: Maximum number of results to return (default: 5)
                
            Returns:
                Formatted search results with relevant document chunks
                and source references
            """
            return self.search_documents(query, n_results)
        
        @tool
        def index_document(file_path: str) -> str:
            """Index a document for semantic search.
            
            Reads a document file, splits it into searchable chunks,
            generates embeddings, and stores in the local vector database.
            
            Args:
                file_path: Path to the document file to index
                          (supports TXT, PDF, Markdown formats)
                
            Returns:
                Status message indicating success or failure with details
            """
            return self.index_document(file_path)
        
        return [search_documents, index_document]
